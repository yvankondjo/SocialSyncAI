# Documentation d'impl√©mentation PRD2 - Confidence Score & Escalade Humain

## Vue d'ensemble

Cette documentation d√©taille l'impl√©mentation compl√®te du PRD2 qui introduit :
- **Format JSON strict** pour toutes les r√©ponses finales du LLM
- **Escalade automatique** vers un humain si confidence < 0.10
- **Gestion FAQ avec questions[]** (liste de formulations multiples)
- **Contr√¥les IA** globaux et par conversation
- **Service find_answers pilot√© par LLM** (sans embeddings)

## üóÉÔ∏è Modifications de la base de donn√©es

### Migration 004 - PRD2

**Fichier**: `backend/migrations/migration_004_prd2_confidence_escalation.sql`

#### A. FAQ - Passage de `question` √† `questions[]`

```sql
-- Ajouter la colonne questions[] 
ALTER TABLE faq_qa
  ADD COLUMN IF NOT EXISTS questions TEXT[] NOT NULL DEFAULT '{}'::text[];

-- Migrer les donn√©es existantes
UPDATE faq_qa 
SET questions = ARRAY[question] 
WHERE question IS NOT NULL AND questions = '{}'::text[];

-- Supprimer l'ancienne colonne et le full-text search
DROP TRIGGER IF EXISTS trg_faq_tsv_update ON faq_qa;
ALTER TABLE faq_qa DROP COLUMN IF EXISTS content;
ALTER TABLE faq_qa DROP COLUMN IF EXISTS question;
ALTER TABLE faq_qa DROP COLUMN IF EXISTS tsv;
```

**Impact** : Chaque FAQ peut maintenant avoir plusieurs formulations de questions au lieu d'une seule.

#### B. Contr√¥le IA par conversation

```sql
ALTER TABLE conversations
  ADD COLUMN IF NOT EXISTS ai_mode TEXT NOT NULL DEFAULT 'ON'
  CHECK (ai_mode IN ('ON', 'OFF'));
```

**Impact** : Permet de d√©sactiver l'IA pour une conversation sp√©cifique apr√®s escalade.

#### C. Table des escalades

```sql
CREATE TABLE IF NOT EXISTS support_escalations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
  message_id UUID NOT NULL,
  confidence DOUBLE PRECISION NOT NULL,
  reason TEXT,
  notified BOOLEAN DEFAULT FALSE,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Impact** : Tra√ßabilit√© compl√®te des escalades avec lien vers la conversation et le message d√©clencheur.

## üìä Sch√©mas Pydantic mis √† jour

### Fichier: `backend/app/schemas/faq_qa_service.py`

#### Nouveaux sch√©mas FAQ

```python
class FAQQA(BaseModel):
    questions: List[str] = Field(default_factory=list)  # Changement majeur
    # ... autres champs

class FAQQuestionsAddRequest(BaseModel):
    items: List[str] = Field(..., min_items=1)

class FAQQuestionsUpdateRequest(BaseModel):
    updates: List[Dict[str, Any]] = Field(..., description="[{index: int, value: str}]")

class FAQQuestionsDeleteRequest(BaseModel):
    indexes: List[int] = Field(..., min_items=1)
```

#### Sch√©mas pour escalades et contr√¥le IA

```python
class SupportEscalation(BaseModel):
    id: UUID
    conversation_id: UUID
    confidence: float
    reason: Optional[str] = None
    notified: bool = False

class ConversationAIModeRequest(BaseModel):
    mode: str = Field(..., description="ON ou OFF")
```

## üõ£Ô∏è Nouvelles routes API

### 1. Routes FAQ - Gestion des questions multiples

#### POST `/api/faq-qa/{faq_id}/questions:add`
**Objectif** : Ajouter des questions √† une FAQ existante

```python
@router.post("/{faq_id}/questions:add")
async def add_faq_questions(
    faq_id: UUID,
    request: FAQQuestionsAddRequest,  # {"items": ["Question 1", "Question 2"]}
    db: Client = Depends(get_authenticated_db),
    current_user_id: str = Depends(get_current_user_id)
):
```

**Exemple d'utilisation** :
```bash
curl -X POST "/api/faq-qa/123e4567-e89b-12d3-a456-426614174000/questions:add" \
  -H "Content-Type: application/json" \
  -d '{"items": ["Comment puis-je faire X?", "Quelle est la proc√©dure pour X?"]}'
```

**R√©ponse** :
```json
{
  "message": "2 questions ajout√©es avec succ√®s",
  "questions": ["Question existante", "Comment puis-je faire X?", "Quelle est la proc√©dure pour X?"]
}
```

#### POST `/api/faq-qa/{faq_id}/questions:update`
**Objectif** : Mettre √† jour des questions sp√©cifiques par index

```python
@router.post("/{faq_id}/questions:update")
async def update_faq_questions(
    faq_id: UUID,
    request: FAQQuestionsUpdateRequest,
    # ...
):
```

**Exemple d'utilisation** :
```bash
curl -X POST "/api/faq-qa/123e4567-e89b-12d3-a456-426614174000/questions:update" \
  -H "Content-Type: application/json" \
  -d '{
    "updates": [
      {"index": 0, "value": "Nouvelle formulation pour la question 1"},
      {"index": 2, "value": "Nouvelle formulation pour la question 3"}
    ]
  }'
```

**Logique** :
- Validation que tous les index sont dans les bornes
- Mise √† jour atomique de toutes les questions sp√©cifi√©es
- Retour de la liste compl√®te mise √† jour

#### POST `/api/faq-qa/{faq_id}/questions:delete`
**Objectif** : Supprimer des questions par index (1..n √©l√©ments)

```python
@router.post("/{faq_id}/questions:delete")
async def delete_faq_questions(
    faq_id: UUID,
    request: FAQQuestionsDeleteRequest,  # {"indexes": [0, 2, 4]}
    # ...
):
```

**Logique critique** :
- Validation des bornes pour tous les index
- Suppression en ordre d√©croissant pour √©viter les d√©calages d'index
- V√©rification qu'il reste au moins une question apr√®s suppression
- Support des index non contigus

### 2. Route contr√¥le IA par conversation

#### PATCH `/api/conversations/{conversation_id}/ai_mode`
**Objectif** : Activer/d√©sactiver l'IA pour une conversation sp√©cifique

```python
@router.patch("/{conversation_id}/ai_mode")
async def update_conversation_ai_mode(
    conversation_id: str,
    request: ConversationAIModeRequest,  # {"mode": "OFF"}
    # ...
):
```

**Cas d'usage** :
- Apr√®s escalade automatique ‚Üí mode "OFF"
- R√©activation manuelle par l'agent ‚Üí mode "ON"
- Interface UI avec banni√®re "Escalad√©e au support"

### 3. Routes support - Gestion des escalades

#### GET `/api/support/escalations`
**Objectif** : Lister toutes les escalades de l'utilisateur

```python
@router.get("/escalations", response_model=List[SupportEscalation])
async def get_user_escalations(
    current_user_id: str = Depends(get_current_user_id),
    # ...
):
```

#### POST `/api/support/escalations/{escalation_id}/notify`
**Objectif** : Marquer une escalade comme notifi√©e et d√©clencher l'email

```python
@router.post("/escalations/{escalation_id}/notify")
async def notify_escalation(
    escalation_id: str,
    # ...
):
```

**Processus** :
1. V√©rification que l'escalade appartient √† l'utilisateur
2. Marquage `notified = true`
3. D√©clenchement de l'envoi d'email (TODO √† impl√©menter)

## üîß Service find_answers PRD2

### Fichier: `backend/app/services/find_answers_prd2.py`

#### Principe architectural

Le nouveau service **abandonne compl√®tement les embeddings** et utilise uniquement le LLM pour la s√©lection :

1. **R√©cup√©ration** : Toutes les FAQs actives du tenant
2. **Pagination** : Division en lots de 20-50 FAQs pour √©viter un contexte trop long
3. **S√©lection LLM** : Pour chaque lot, le LLM choisit la meilleure correspondance
4. **Agr√©gation** : S√©lection du meilleur r√©sultat parmi tous les lots

#### Impl√©mentation d√©taill√©e

```python
class FindAnswersService:
    def __init__(self, user_id: str, batch_size: int = 30):
        self.user_id = user_id
        self.batch_size = batch_size
    
    def find_answers(self, question: str) -> Answer:
        # 1. R√©cup√©rer toutes les FAQs actives
        faqs = self.get_active_faqs()
        
        # 2. Diviser en lots
        batches = self.create_batches(faqs)
        
        # 3. Traiter chaque lot avec le LLM
        batch_results = []
        for batch in batches:
            result = self.process_batch(batch, question)
            if result:
                batch_results.append(result)
        
        # 4. S√©lectionner le meilleur r√©sultat
        best_match = self.find_best_across_batches(batch_results)
        
        # 5. Retourner un objet Answer structur√©
        return self.build_answer(best_match, faqs)
```

#### Format de prompt LLM

```python
prompt = f"""Tu es un assistant sp√©cialis√© dans la recherche de FAQs.

Voici une question utilisateur: "{user_question}"

Voici une liste de FAQs disponibles:
{json.dumps(faqs_context, ensure_ascii=False, indent=2)}

R√©ponds UNIQUEMENT avec un JSON dans ce format:
{{
  "best_match": {{
    "faq_id": "uuid",
    "matched_question_index": 0,
    "matched_question_text": "texte exact",
    "confidence_local": 0.85,
    "reason": "explication courte"
  }}
}}

OU si aucune FAQ ne correspond:
{{
  "no_match": {{
    "reason": "explication courte"
  }}
}}
"""
```

#### Objet Answer retourn√©

```python
class Answer(BaseModel):
    faq_id: Optional[str] = None
    answer: Optional[str] = None
    matched_question_index: Optional[int] = None
    matched_question_text: Optional[str] = None
    reason: str
    status: str  # "match" | "no_match"
```

**Important** : Cet objet Answer n'est PAS la r√©ponse finale √† l'utilisateur, mais sert de contexte au LLM g√©n√©ratif principal.

## ü§ñ Agent RAG PRD2

### Fichier: `backend/app/services/rag_agent_prd2.py`

#### R√®gles de sortie strictes

L'agent impl√©mente les r√®gles PRD2 :

```python
base_system_prompt = """
R√àGLES DE SORTIE STRICTES :
- Si tu r√©ponds √† l'utilisateur, renvoie UNIQUEMENT :
  {"response":"<texte>", "confidence": <float 0..1>}
- Si tu dois utiliser un outil, utilise le tool-calling natif. 
  Le tour suivant DOIT √™tre la r√©ponse finale JSON ci-dessus.
- Ne m√©lange jamais tool-call et JSON final dans le m√™me tour.
- Si on te renvoie "JSON_INVALID", renvoie uniquement le JSON corrig√©.
"""
```

#### Gestion des Gates IA

```python
def process_message(self, user_message: str, conversation_id: str, message_id: str = None):
    # Gate 1: IA globale d√©sactiv√©e
    if not self.check_ai_settings():
        return {"response": None, "reason": "IA globalement d√©sactiv√©e"}
    
    # Gate 2: IA d√©sactiv√©e pour cette conversation
    if not self.check_conversation_ai_mode(conversation_id):
        return {"response": None, "reason": "IA d√©sactiv√©e pour cette conversation"}
    
    # ... traitement normal
```

#### Parsing JSON avec retry

```python
def parse_or_fix_json(self, response_text: str) -> Optional[AIResponse]:
    try:
        # Premi√®re tentative
        response_json = json.loads(response_text.strip())
        return AIResponse(**response_json)
    except (json.JSONDecodeError, ValidationError):
        # Retry automatique avec correction LLM
        return self.retry_json_correction(response_text)
```

#### Escalade automatique

```python
# V√©rifier le seuil d'escalade
if ai_response.confidence < 0.10:
    escalation_id = self.create_escalation(
        conversation_id=conversation_id,
        message_id=message_id,
        confidence=ai_response.confidence,
        reason=f"Confiance faible: {ai_response.confidence}"
    )
    
    return {
        "response": "Transfert √† un agent humain.",
        "escalated": True,
        "escalation_id": escalation_id
    }
```

#### Gestion des tools

Le syst√®me supporte le tool-calling natif :

1. **Premier appel LLM** : Peut d√©cider d'utiliser `find_answers`
2. **Ex√©cution du tool** : R√©cup√©ration des donn√©es FAQ
3. **Second appel LLM** : DOIT produire la r√©ponse JSON finale

## üß™ Tests complets

### Tests unitaires (`backend/tests/test_prd2_features.py`)

#### Tests JSON parsing
- ‚úÖ JSON valide ‚Üí OK
- ‚úÖ JSON invalide ‚Üí retry "JSON_INVALID" ‚Üí OK  
- ‚úÖ JSON invalide apr√®s retry ‚Üí erreur contr√¥l√©e

#### Tests seuil escalade
- ‚úÖ `confidence = 0.09` ‚Üí escalade + `ai_mode='OFF'`
- ‚úÖ `confidence = 0.10` ‚Üí pas d'escalade

#### Tests tool-calling
- ‚úÖ Pr√©sence de `.tool_calls` ‚Üí aucun parsing JSON tent√©

#### Tests FAQ operations
- ‚úÖ Add multiples questions
- ‚úÖ Update par index (validation bornes)
- ‚úÖ Delete 1..n indexes (indexes non contigus)

### Tests d'int√©gration

#### Tests Gates IA
- ‚úÖ `ai_settings.is_active=false` ‚Üí LLM non appel√©
- ‚úÖ `conversations.ai_mode='OFF'` ‚Üí LLM non appel√© pour ce thread

#### Tests escalade
- ‚úÖ Insertion `support_escalations` + mutation `ai_mode='OFF'`
- ‚úÖ Email marqu√© `notified=true` (optionnel)

### Tests E2E simul√©s

#### Sc√©nario normal
1. Message utilisateur ‚Üí tool-call (find_answers) 
2. ‚Üí JSON final `{response, confidence=0.42}` 
3. ‚Üí message livr√©

#### Sc√©nario escalade
1. Message utilisateur ‚Üí JSON final `{..., confidence=0.05}` 
2. ‚Üí escalade + mute + banni√®re UI

### Tests routes API (`backend/tests/test_routers/test_faq_prd2_routes.py`)

- ‚úÖ Routes FAQ questions:add/update/delete
- ‚úÖ Route conversation ai_mode
- ‚úÖ Routes support escalations
- ‚úÖ Validation des erreurs et cas limites

## üîí S√©curit√© et RLS

### Row Level Security (RLS)

Toutes les nouvelles tables respectent le principe RLS :

```sql
-- support_escalations
CREATE POLICY "Users manage their escalations" ON support_escalations 
FOR ALL USING (auth.uid() = user_id);

-- ai_settings  
CREATE POLICY "Users manage their ai_settings" ON ai_settings 
FOR ALL USING (auth.uid() = user_id);
```

### Validation des donn√©es

- **FAQ questions** : Validation des index pour update/delete
- **AI mode** : Validation stricte "ON" | "OFF"
- **Escalades** : V√©rification appartenance utilisateur
- **Limites** : Taille maximale `questions[]` pour √©viter inflation prompts

## üìà Observabilit√©

### Logs recommand√©s

```python
# JSON invalide + retry
logger.warning(f"JSON invalide, tentative de correction: {e}")

# Escalades
logger.info(f"Escalade cr√©√©e: {escalation_id} pour conversation {conversation_id}")

# Performance find_answers
logger.info(f"Traitement de {len(faqs)} FAQs en {len(batches)} lots")
```

### M√©triques √† surveiller

- **% escalades** : Ratio escalades / messages totaux
- **Temps moyen find_answers** : Performance par lot
- **Taille moyenne prompt** : Optimisation batch_size
- **Taux retry JSON** : Qualit√© des r√©ponses LLM

## üöÄ D√©ploiement

### Ordre d'ex√©cution

1. **Migration SQL** : `migration_004_prd2_confidence_escalation.sql`
2. **D√©ploiement backend** : Nouvelles routes et services
3. **Tests** : Validation suite compl√®te
4. **Configuration** : Variables d'environnement LLM
5. **Monitoring** : Activation logs et m√©triques

### Variables d'environnement requises

```bash
OPENROUTER_API_KEY=your_api_key
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

### Points d'attention

- **Batch size** : Ajuster selon la taille moyenne des FAQs
- **Seuil escalade** : 0.10 par d√©faut, configurable si n√©cessaire
- **Performance** : Surveiller temps de r√©ponse avec lots multiples
- **Co√ªts LLM** : Optimiser taille des prompts et fr√©quence d'appels

## üìã Crit√®res d'acceptation - Validation

- ‚úÖ Le LLM ne sort **jamais** autre chose que `{response, confidence}` pour la r√©ponse finale
- ‚úÖ `confidence < 0.10` ‚Üí `conversations.ai_mode='OFF'` + ligne `support_escalations` cr√©√©e
- ‚úÖ `find_answers` fonctionne **sans embeddings**, via LLM, avec lots si n√©cessaire
- ‚úÖ FAQ : **add/update/delete (multi)** op√©rationnels avec validation compl√®te
- ‚úÖ `ai_settings.is_active=false` ‚Üí IA muette globalement
- ‚úÖ Suite de **tests verte** avec couverture unitaire, int√©gration et E2E

## üîÑ √âvolutions futures

### Phase 2 - Am√©liorations

- **Email automatique** : Impl√©mentation compl√®te envoi notifications escalade
- **UI avanc√©e** : Interface de gestion des escalades avec filtres et recherche
- **Analytics** : Dashboard de performance IA et taux d'escalade
- **Optimisations** : Cache LLM pour r√©ponses fr√©quentes, batch size dynamique

### Phase 3 - Extensions

- **Multi-langues** : Support escalades dans diff√©rentes langues
- **R√®gles m√©tier** : Escalades conditionnelles selon contexte client
- **Int√©grations** : Webhooks vers syst√®mes de ticketing externes
- **IA adaptative** : Ajustement automatique du seuil selon performance

---

Cette impl√©mentation respecte int√©gralement les sp√©cifications du PRD2 et fournit une base solide pour les √©volutions futures du syst√®me d'IA conversationnelle avec escalade humaine.
